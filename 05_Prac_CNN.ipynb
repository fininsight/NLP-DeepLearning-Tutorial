{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "05 Prac. CNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fininsight/nlp-deeplearning-tutorial/blob/master/05_Prac_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9THKomKIxvC",
        "colab_type": "text"
      },
      "source": [
        "# 5장. CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVu3zcxEOq6a",
        "colab_type": "text"
      },
      "source": [
        "#1 CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fG9HYp3-I5sg",
        "colab_type": "text"
      },
      "source": [
        "https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/ko/tutorials/images/cnn.ipynb?hl=ko#scrollTo=jKgyC5K_4O0d"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jRFxccghyMVo"
      },
      "source": [
        "###1) MNIST 데이터셋 다운로드하고 준비하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JWoEqyMuXFF4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "87a57ef3-4e49-4384-ad81-22e11dfa8e85"
      },
      "source": [
        "#!pip install tensorflow-gpu==2.0.0-rc1\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = datasets.mnist.load_data()\n",
        "\n",
        "train_images = train_images.reshape((60000, 28, 28, 1)) #데이터 건수, 이미지 높이, 이미지 너비, 컬러 채널\n",
        "test_images = test_images.reshape((10000, 28, 28, 1)) #데이터 건수, 이미지 높이, 이미지 너비, 컬러 채널\n",
        "\n",
        "# 픽셀 값을 0~1 사이로 정규화합니다.\n",
        "train_images, test_images = train_images / 255.0, test_images / 255.0"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Oewp-wYg31t9"
      },
      "source": [
        "### 2) CNN 만들기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "L9YmGQBQPrdn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "3eef88ef-d70a-4603-cfce-977988a5f26c"
      },
      "source": [
        "model = models.Sequential()\n",
        "# 특징 추출 (Feature Extraction)\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1))) # 32 * 3 * 3 + 32 = 320\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu')) # 32 * 64 * 3 * 3 + 64 = 18496\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu')) # 64 * 64 * 3 * 3 + 64 = 36928\n",
        "\n",
        "# 분류 (Classification)\n",
        "model.add(layers.Flatten()) # 576개 벡터로 Flatten\n",
        "model.add(layers.Dense(64, activation='relu')) # 576 * 64 + 64 = 36928\n",
        "model.add(layers.Dense(10, activation='softmax')) # 64 * 10 + 10 = 650\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 11, 11, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 3, 3, 64)          36928     \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 576)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 64)                36928     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                650       \n",
            "=================================================================\n",
            "Total params: 93,322\n",
            "Trainable params: 93,322\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "P3odqfHP4M67"
      },
      "source": [
        "### 3) 모델 컴파일과 훈련하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MdDzI75PUXrG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "d0f50180-c5c1-420f-d8a6-8c0f45a3dccf"
      },
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(train_images, train_labels, epochs=5)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1440 - accuracy: 0.9566\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0438 - accuracy: 0.9868\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0310 - accuracy: 0.9905\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0231 - accuracy: 0.9926\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0176 - accuracy: 0.9944\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f6520166da0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jKgyC5K_4O0d"
      },
      "source": [
        "###4) 모델 평가"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gtyDF0MKUcM7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "501fe25c-4beb-4a17-cab5-bc887512e6fa"
      },
      "source": [
        "test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)\n",
        "print(test_acc)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 - 0s - loss: 0.0303 - accuracy: 0.9903\n",
            "0.9902999997138977\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUJN6pqEOaIy",
        "colab_type": "text"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bvGKMdSOVyC",
        "colab_type": "text"
      },
      "source": [
        "# 2 CNN for Sentence Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWr3mGouyFmQ",
        "colab_type": "text"
      },
      "source": [
        "https://www.aclweb.org/anthology/D14-1181/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUy7jArNyMZV",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"http://www.wildml.com/wp-content/uploads/2015/11/Screen-Shot-2015-11-06-at-8.03.47-AM-1024x413.png\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCJC8o7KfTNN",
        "colab_type": "text"
      },
      "source": [
        "##1) 네이버 영화 리뷰 다운로드"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ac0ZajKK2Ph_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 820
        },
        "outputId": "a5f18170-a470-40c9-a6b5-09be682a1360"
      },
      "source": [
        "!pip install tensorflow-gpu "
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/11/763f55d3d15efd778ef24453f126e6c33635680e5a2bb346da3fab5997cb/tensorflow_gpu-2.3.0-cp36-cp36m-manylinux2010_x86_64.whl (320.4MB)\n",
            "\u001b[K     |████████████████████████████████| 320.4MB 50kB/s \n",
            "\u001b[?25hRequirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.6.3)\n",
            "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.18.5)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.1.2)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (3.12.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.2.0)\n",
            "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.4.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.31.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.34.2)\n",
            "Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (2.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.12.1)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (2.10.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.9.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (3.3.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (2.3.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.15.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.9.2->tensorflow-gpu) (49.2.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu) (0.4.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu) (3.2.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu) (2.23.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu) (1.17.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu) (1.7.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu) (1.0.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow-gpu) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow-gpu) (1.7.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow-gpu) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow-gpu) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow-gpu) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow-gpu) (2020.6.20)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow-gpu) (4.1.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow-gpu) (4.6)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow-gpu) (0.2.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow-gpu) (3.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow-gpu) (3.1.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow-gpu) (0.4.8)\n",
            "Installing collected packages: tensorflow-gpu\n",
            "Successfully installed tensorflow-gpu-2.3.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tensorflow"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNriA4DKNVgQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "outputId": "bf69c22b-e52b-4d8b-f19e-a5add81149a4"
      },
      "source": [
        "# 네이버 영화 리뷰 다운로드\n",
        "!wget https://github.com/e9t/nsmc/raw/master/ratings.txt\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "df = pd.read_csv(\"./ratings.txt\",sep='\\t').dropna()\n",
        "df.head(5)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-08-23 10:32:08--  https://github.com/e9t/nsmc/raw/master/ratings.txt\n",
            "Resolving github.com (github.com)... 140.82.112.4\n",
            "Connecting to github.com (github.com)|140.82.112.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/e9t/nsmc/master/ratings.txt [following]\n",
            "--2020-08-23 10:32:08--  https://raw.githubusercontent.com/e9t/nsmc/master/ratings.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 19515078 (19M) [text/plain]\n",
            "Saving to: ‘ratings.txt.1’\n",
            "\n",
            "ratings.txt.1       100%[===================>]  18.61M  29.5MB/s    in 0.6s    \n",
            "\n",
            "2020-08-23 10:32:09 (29.5 MB/s) - ‘ratings.txt.1’ saved [19515078/19515078]\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>document</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>8112052</td>\n",
              "      <td>어릴때보고 지금다시봐도 재밌어요ㅋㅋ</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>8132799</td>\n",
              "      <td>디자인을 배우는 학생으로, 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4655635</td>\n",
              "      <td>폴리스스토리 시리즈는 1부터 뉴까지 버릴께 하나도 없음.. 최고.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9251303</td>\n",
              "      <td>와.. 연기가 진짜 개쩔구나.. 지루할거라고 생각했는데 몰입해서 봤다.. 그래 이런...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>10067386</td>\n",
              "      <td>안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         id                                           document  label\n",
              "0   8112052                                어릴때보고 지금다시봐도 재밌어요ㅋㅋ      1\n",
              "1   8132799  디자인을 배우는 학생으로, 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산...      1\n",
              "2   4655635               폴리스스토리 시리즈는 1부터 뉴까지 버릴께 하나도 없음.. 최고.      1\n",
              "3   9251303  와.. 연기가 진짜 개쩔구나.. 지루할거라고 생각했는데 몰입해서 봤다.. 그래 이런...      1\n",
              "4  10067386                        안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화.      1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rfddH3RfXLe",
        "colab_type": "text"
      },
      "source": [
        "##2) 전처리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPgNyyoXkB6f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "from tensorflow.keras import preprocessing\n",
        "\n",
        "def preprocess(x, y, padding_size = 128, oov_token=\"<UNK>\", vocab_file = \"vocab.json\", train_ratio=0.7) :  \n",
        "  preprocessor = preprocessing.text.Tokenizer(oov_token=oov_token) #토큰화\n",
        "  preprocessor.fit_on_texts(x)\n",
        "  x = preprocessor.texts_to_sequences(x) #시퀀스로 변환\n",
        "  vocab = preprocessor.word_index #단어:인덱스 dictionary\n",
        "  json.dump(vocab, open(vocab_file, 'w'), ensure_ascii=False)\n",
        "  x = preprocessing.sequence.pad_sequences(x, maxlen=padding_size, padding='post', truncating='post')\n",
        "\n",
        "  index = int(len(x)*train_ratio)\n",
        "\n",
        "  return x[:index], y[:index], x[index:], y[index:], vocab\n",
        "\n",
        "x_train, y_train, x_test, y_test, vocab = preprocess(df[\"document\"].tolist(), df[\"label\"].tolist())"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzkqF56qfavV",
        "colab_type": "text"
      },
      "source": [
        "##3) 모델 정의"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrSyfNITYP6U",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"https://miro.medium.com/max/770/0*wigQtmJiv0bddwPI.\" width=\"500\" />"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sx6gRwN6Oczp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629
        },
        "outputId": "c31cf21f-e352-47ea-92ab-a05f46a5930b"
      },
      "source": [
        "from tensorflow import keras\n",
        "\n",
        "def CNNforText( num_classes,  #클래스 갯수\n",
        "          vocab_size,\n",
        "          embed_size = 512, #단어 임베딩 사이즈                 \n",
        "          filter_sizes = [3,4,5],\n",
        "          regularizers_lambda = 0.01, #L2 regulation parameter\n",
        "          dropout =  0.5,\n",
        "          feature_size = 128, #문장 시퀀스 길이\n",
        "          num_filters = 128 #필터 개수 (필터사이즈와 같음). mhlee 하나로 통일하자\n",
        ") :\n",
        "          \n",
        "\n",
        "  inputs = keras.Input(shape=(feature_size,), name='input_data')\n",
        "  embed_initer = keras.initializers.RandomUniform(minval=-1, maxval=1)\n",
        "  #sequence 임베딩\n",
        "  embed = keras.layers.Embedding(vocab_size, embed_size,\n",
        "                                  embeddings_initializer=embed_initer,\n",
        "                                  input_length=feature_size,\n",
        "                                  name='embedding')(inputs)\n",
        "                                  \n",
        "  embed = keras.layers.Reshape((feature_size, embed_size, 1), name='add_channel')(embed)\n",
        "\n",
        "  pool_outputs = []\n",
        "\n",
        "  #filter 별로 모델 구성\n",
        "  for filter_size in filter_sizes :\n",
        "    #convolution\n",
        "    filter_shape = (filter_size, embed_size)\n",
        "    conv = keras.layers.Conv2D(num_filters, filter_shape, strides=(1, 1), padding='valid',\n",
        "                                data_format='channels_last', activation='relu',\n",
        "                                kernel_initializer='glorot_normal',\n",
        "                                bias_initializer=keras.initializers.constant(0.1),\n",
        "                                name='convolution_{:d}'.format(filter_size))(embed)\n",
        "    #max pooling\n",
        "    max_pool_shape = (feature_size - filter_size + 1, 1)\n",
        "    pool = keras.layers.MaxPool2D(pool_size=max_pool_shape,\n",
        "                                  strides=(1, 1), padding='valid',\n",
        "                                  data_format='channels_last',\n",
        "                                  name='max_pooling_{:d}'.format(filter_size))(conv)\n",
        "    pool_outputs.append(pool)\n",
        "\n",
        "  pool_outputs = keras.layers.concatenate(pool_outputs, axis=-1, name='concatenate')\n",
        "  pool_outputs = keras.layers.Flatten(data_format='channels_last', name='flatten')(pool_outputs)\n",
        "  pool_outputs = keras.layers.Dropout(dropout, name='dropout')(pool_outputs)\n",
        "\n",
        "  outputs = keras.layers.Dense(num_classes, activation='softmax',\n",
        "                                kernel_initializer='glorot_normal',\n",
        "                                bias_initializer=keras.initializers.constant(0.1),\n",
        "                                kernel_regularizer=keras.regularizers.l2(regularizers_lambda),\n",
        "                                bias_regularizer=keras.regularizers.l2(regularizers_lambda),\n",
        "                                name='dense')(pool_outputs)\n",
        "  model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "  model.summary()\n",
        "  return model, num_classes\n",
        "\n",
        "model, num_classes = CNNforText(len(np.unique(y_train)), len(vocab))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_data (InputLayer)         [(None, 128)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 128, 512)     188943872   input_data[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_channel (Reshape)           (None, 128, 512, 1)  0           embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "convolution_3 (Conv2D)          (None, 126, 1, 128)  196736      add_channel[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "convolution_4 (Conv2D)          (None, 125, 1, 128)  262272      add_channel[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "convolution_5 (Conv2D)          (None, 124, 1, 128)  327808      add_channel[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling_3 (MaxPooling2D)    (None, 1, 1, 128)    0           convolution_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling_4 (MaxPooling2D)    (None, 1, 1, 128)    0           convolution_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling_5 (MaxPooling2D)    (None, 1, 1, 128)    0           convolution_5[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 1, 1, 384)    0           max_pooling_3[0][0]              \n",
            "                                                                 max_pooling_4[0][0]              \n",
            "                                                                 max_pooling_5[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 384)          0           concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 384)          0           flatten[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 2)            770         dropout[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 189,731,458\n",
            "Trainable params: 189,731,458\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLYN-56TfrPS",
        "colab_type": "text"
      },
      "source": [
        "##4) 학습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdwIPx2rdEmA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "efc6f4f7-47f8-4c4c-d0a0-37549c98a2b2"
      },
      "source": [
        "import os\n",
        "import time\n",
        "import tensorflow as tf\n",
        "\n",
        "def train(model, x_train, y_train, num_classes\n",
        "          , batch_size = 64, epochs = 1, fraction_validation = 0.05, results_dir = \"./result/\", save_path = \"model\") :\n",
        "  timestamp = time.strftime(\"%Y-%m-%d-%H-%M\", time.localtime(time.time()))\n",
        "  path = os.path.join(results_dir, timestamp)\n",
        "  if not os.path.exists(path) :    \n",
        "    path_log = os.path.join(path, 'log/')\n",
        "    os.makedirs(path_log)\n",
        "\n",
        "  model.compile(tf.optimizers.Adam(), loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "  #모델 구조 이미지 파일로 저장\n",
        "  keras.utils.plot_model(model, show_shapes=True, to_file=os.path.join(path, \"model.jpg\"))\n",
        "  y_train = tf.one_hot(y_train, num_classes)\n",
        "  tb_callback = keras.callbacks.TensorBoard(path_log,\n",
        "                                            histogram_freq=0.1, write_graph=True,\n",
        "                                            write_images=True,\n",
        "                                            embeddings_freq=0.5, update_freq='batch')\n",
        "\n",
        "  history = model.fit(x=x_train, y=y_train, batch_size=batch_size, epochs=epochs,\n",
        "                                callbacks=[tb_callback], validation_split=fraction_validation, shuffle=True)\n",
        "  \n",
        "  #모델 저장\n",
        "  keras.models.save_model(model, save_path)\n",
        "  print(history.history)\n",
        "\n",
        "  return model, path_log\n",
        "\n",
        "model, path_log = train(model, x_train, y_train, num_classes)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r   1/2079 [..............................] - ETA: 1s - loss: 0.7176 - accuracy: 0.5938WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
            "Instructions for updating:\n",
            "use `tf.profiler.experimental.stop` instead.\n",
            "2079/2079 [==============================] - ETA: 0s - loss: 0.4451 - accuracy: 0.8044"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxKDmHsEwLmH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir {path_log}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1brKOSoNpqE8",
        "colab_type": "text"
      },
      "source": [
        "##5) 평가"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SuKxvAvMps3J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
        "\n",
        "def test(model, x_test, y_test, num_classes):\n",
        "  y_pred_one_hot = model.predict(x=x_test, batch_size=1, verbose=1)\n",
        "  y_pred = tf.math.argmax(y_pred_one_hot, axis=1)\n",
        "\n",
        "  print('\\nTest accuracy: {}\\n'.format(accuracy_score(y_test, y_pred)))\n",
        "  print('Classification report:')\n",
        "  target_names = ['class {:d}'.format(i) for i in np.arange(num_classes)]\n",
        "  print(classification_report(y_test, y_pred, target_names=target_names, digits=4))\n",
        "\n",
        "test(model, x_test, y_test, num_classes)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}