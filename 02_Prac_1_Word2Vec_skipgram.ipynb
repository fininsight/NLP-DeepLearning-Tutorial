{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "02 Prac 1. Word2Vec - skipgram.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyORX0AVnF8NGAye9cVuf9+2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fininsight/nlp-deeplearning-tutorial/blob/master/02_Prac_1_Word2Vec_skipgram.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEg0-L6s_3wS",
        "colab_type": "text"
      },
      "source": [
        "#Word Embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dijqDawl_9eJ",
        "colab_type": "text"
      },
      "source": [
        "#1 Word2Vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-18xOwVAAF-",
        "colab_type": "text"
      },
      "source": [
        "##1.1 Skip-gram 직접구현"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YA-gZmHA_xQQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "docs = [\"natural language processing and machine learning is fun and exciting\",\n",
        "        \"natural language processing and machine learning\"]"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9zj1XGtAJAq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "b8904715-c6d7-49f6-e824-ddf861c055b1"
      },
      "source": [
        "from collections import defaultdict\n",
        "import numpy as np \n",
        "\n",
        "class OneHotEncoder() :\n",
        "  def __init__(self, docs) :\n",
        "    # 고유 단어와 인덱스를 매칭시켜주는 사전 생성\n",
        "    self.w2i = defaultdict(lambda:len(self.w2i))\n",
        "    [self.w2i[w] for d in docs for w in d]\n",
        "    self.i2w = {v:k for k,v in self.w2i.items()}\n",
        "\n",
        "  def _get_one_hot_vector(self, w):\n",
        "    v = [0]*len(self.w2i)\n",
        "    v[self.w2i[w]] = 1\n",
        "    return v\n",
        "\n",
        "  def encode(self, docs) :\n",
        "    ret = []\n",
        "    for d in docs :\n",
        "      tmp = []\n",
        "      for w in d :\n",
        "        tmp.append(self._get_one_hot_vector(w))\n",
        "      ret.append(tmp)\n",
        "    return ret\n",
        "\n",
        "  def decode(self, v) :\n",
        "    return self.i2w[v.index(1)]\n",
        "\n",
        "tokenized_docs = [d.split() for d in docs]\n",
        "ohe = OneHotEncoder(tokenized_docs)\n",
        "encoded_docs = ohe.encode(tokenized_docs)\n",
        "encoded_docs"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[[1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "  [0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
              "  [0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
              "  [0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
              "  [0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
              "  [0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
              "  [0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
              "  [0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
              "  [0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
              "  [0, 0, 0, 0, 0, 0, 0, 0, 1]],\n",
              " [[1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "  [0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
              "  [0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
              "  [0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
              "  [0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
              "  [0, 0, 0, 0, 0, 1, 0, 0, 0]]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FX0iaQkFBtx2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def slide_window(encoded_docs) :\n",
        "  win_size = 2\n",
        "  ret = []\n",
        "\n",
        "  for d in encoded_docs : \n",
        "    win_doc = []\n",
        "    for i, w in enumerate(d) :\n",
        "      s, e = max(0, i - win_size), min(len(d), i + win_size)\n",
        "      context = d[s:e+1]\n",
        "      center = context.pop(i-s)\n",
        "      win_doc.append((center, context))\n",
        "    ret.append(win_doc)\n",
        "  return ret"
      ],
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qru8FFY-sba",
        "colab_type": "text"
      },
      "source": [
        "![대체 텍스트](https://miro.medium.com/max/1400/1*uuVrJhSF89KGJPltvJ4xhg.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6617dgo7HjO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def softmax(output) :\n",
        "  return np.exp(output) / np.sum(np.exp(output))\n",
        "\n",
        "#SGD to backpropagate errors \n",
        "def backpropagation(W1, W2, hidden, predict, center, context, learning_rate) :\n",
        "    err = (softmax(predict) - context).sum(axis=0)\n",
        "\n",
        "    delta_W2 = np.outer(hidden, err)\n",
        "    delta_W1 = np.outer(center, np.dot(W2, err))\n",
        "\n",
        "    W1 = W1 - learning_rate * delta_W1\n",
        "    W2 = W2 - learning_rate * delta_W2\n",
        "\n",
        "    return W1, W2"
      ],
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UaU9Umq02ylZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c8fc4cd6-5d5e-4ec7-a6a5-9678b62dedc7"
      },
      "source": [
        "onehot_size = len(encoded_docs[0][0])\n",
        "embedding_size = 5\n",
        "learning_rate = 0.001\n",
        "epoch = 2\n",
        "\n",
        "W1 = np.random.rand(onehot_size, embedding_size)\n",
        "W2 = np.random.rand(embedding_size, onehot_size)\n",
        "\n",
        "for i in range(epoch) :\n",
        "  for d in slide_window(encoded_docs) :\n",
        "    for center, context in d : \n",
        "      hidden = np.dot(center, W1)\n",
        "      predict = np.dot(hidden, W2)\n",
        "      W1, W2 = backpropagation(W1, W2, hidden, predict, center, context, learning_rate)\n",
        "\n",
        "    np.log(np.sum(np.exp(u))\n",
        "\n",
        "  self.loss += -np.sum([u[word.index(1)] for word in w_c]) + len(w_c) * np.log(np.sum(np.exp(u)))\n",
        "    \n",
        "\n",
        "      print(W1)\n",
        "\n"
      ],
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.26041504 0.92153019 0.43489931 0.1739126  0.07203218]\n",
            " [0.49973765 0.40210404 0.95331045 0.832181   0.20565733]\n",
            " [0.82901255 0.88172228 0.02798924 0.15746927 0.92429208]\n",
            " [0.2057222  0.67240718 0.70000297 0.09461725 0.13704983]\n",
            " [0.04341349 0.94100868 0.55460107 0.75780731 0.00860506]\n",
            " [0.97816839 0.60889983 0.87136236 0.81359383 0.57016172]\n",
            " [0.29852799 0.55126335 0.35412346 0.72496239 0.10452159]\n",
            " [0.38638445 0.61889755 0.75788279 0.70098857 0.40555984]\n",
            " [0.43624882 0.39230823 0.39652264 0.81799001 0.9861178 ]]\n",
            "[[0.26041504 0.92153019 0.43489931 0.1739126  0.07203218]\n",
            " [0.49903593 0.40291537 0.95332648 0.83176616 0.20560173]\n",
            " [0.82901255 0.88172228 0.02798924 0.15746927 0.92429208]\n",
            " [0.2057222  0.67240718 0.70000297 0.09461725 0.13704983]\n",
            " [0.04341349 0.94100868 0.55460107 0.75780731 0.00860506]\n",
            " [0.97816839 0.60889983 0.87136236 0.81359383 0.57016172]\n",
            " [0.29852799 0.55126335 0.35412346 0.72496239 0.10452159]\n",
            " [0.38638445 0.61889755 0.75788279 0.70098857 0.40555984]\n",
            " [0.43624882 0.39230823 0.39652264 0.81799001 0.9861178 ]]\n",
            "[[0.26041504 0.92153019 0.43489931 0.1739126  0.07203218]\n",
            " [0.49903593 0.40291537 0.95332648 0.83176616 0.20560173]\n",
            " [0.82791787 0.88214472 0.02808915 0.15691164 0.9241367 ]\n",
            " [0.2057222  0.67240718 0.70000297 0.09461725 0.13704983]\n",
            " [0.04341349 0.94100868 0.55460107 0.75780731 0.00860506]\n",
            " [0.97816839 0.60889983 0.87136236 0.81359383 0.57016172]\n",
            " [0.29852799 0.55126335 0.35412346 0.72496239 0.10452159]\n",
            " [0.38638445 0.61889755 0.75788279 0.70098857 0.40555984]\n",
            " [0.43624882 0.39230823 0.39652264 0.81799001 0.9861178 ]]\n",
            "[[0.26041504 0.92153019 0.43489931 0.1739126  0.07203218]\n",
            " [0.49903593 0.40291537 0.95332648 0.83176616 0.20560173]\n",
            " [0.82791787 0.88214472 0.02808915 0.15691164 0.9241367 ]\n",
            " [0.20674612 0.67158467 0.69898128 0.09509231 0.13659004]\n",
            " [0.04341349 0.94100868 0.55460107 0.75780731 0.00860506]\n",
            " [0.97816839 0.60889983 0.87136236 0.81359383 0.57016172]\n",
            " [0.29852799 0.55126335 0.35412346 0.72496239 0.10452159]\n",
            " [0.38638445 0.61889755 0.75788279 0.70098857 0.40555984]\n",
            " [0.43624882 0.39230823 0.39652264 0.81799001 0.9861178 ]]\n",
            "[[0.26041504 0.92153019 0.43489931 0.1739126  0.07203218]\n",
            " [0.49903593 0.40291537 0.95332648 0.83176616 0.20560173]\n",
            " [0.82791787 0.88214472 0.02808915 0.15691164 0.9241367 ]\n",
            " [0.20674612 0.67158467 0.69898128 0.09509231 0.13659004]\n",
            " [0.04378205 0.94071857 0.55442289 0.75814693 0.00797407]\n",
            " [0.97816839 0.60889983 0.87136236 0.81359383 0.57016172]\n",
            " [0.29852799 0.55126335 0.35412346 0.72496239 0.10452159]\n",
            " [0.38638445 0.61889755 0.75788279 0.70098857 0.40555984]\n",
            " [0.43624882 0.39230823 0.39652264 0.81799001 0.9861178 ]]\n",
            "[[0.26041504 0.92153019 0.43489931 0.1739126  0.07203218]\n",
            " [0.49903593 0.40291537 0.95332648 0.83176616 0.20560173]\n",
            " [0.82791787 0.88214472 0.02808915 0.15691164 0.9241367 ]\n",
            " [0.20674612 0.67158467 0.69898128 0.09509231 0.13659004]\n",
            " [0.04378205 0.94071857 0.55442289 0.75814693 0.00797407]\n",
            " [0.97692033 0.6096903  0.87129821 0.8135133  0.56908354]\n",
            " [0.29852799 0.55126335 0.35412346 0.72496239 0.10452159]\n",
            " [0.38638445 0.61889755 0.75788279 0.70098857 0.40555984]\n",
            " [0.43624882 0.39230823 0.39652264 0.81799001 0.9861178 ]]\n",
            "[[0.26041504 0.92153019 0.43489931 0.1739126  0.07203218]\n",
            " [0.49903593 0.40291537 0.95332648 0.83176616 0.20560173]\n",
            " [0.82791787 0.88214472 0.02808915 0.15691164 0.9241367 ]\n",
            " [0.20674612 0.67158467 0.69898128 0.09509231 0.13659004]\n",
            " [0.04378205 0.94071857 0.55442289 0.75814693 0.00797407]\n",
            " [0.97692033 0.6096903  0.87129821 0.8135133  0.56908354]\n",
            " [0.29778104 0.55187308 0.35339633 0.72495755 0.10319394]\n",
            " [0.38638445 0.61889755 0.75788279 0.70098857 0.40555984]\n",
            " [0.43624882 0.39230823 0.39652264 0.81799001 0.9861178 ]]\n",
            "[[0.26041504 0.92153019 0.43489931 0.1739126  0.07203218]\n",
            " [0.49903593 0.40291537 0.95332648 0.83176616 0.20560173]\n",
            " [0.82791787 0.88214472 0.02808915 0.15691164 0.9241367 ]\n",
            " [0.20674612 0.67158467 0.69898128 0.09509231 0.13659004]\n",
            " [0.04378205 0.94071857 0.55442289 0.75814693 0.00797407]\n",
            " [0.97692033 0.6096903  0.87129821 0.8135133  0.56908354]\n",
            " [0.29778104 0.55187308 0.35339633 0.72495755 0.10319394]\n",
            " [0.38649831 0.6186115  0.7578479  0.70111794 0.40498143]\n",
            " [0.43624882 0.39230823 0.39652264 0.81799001 0.9861178 ]]\n",
            "[[0.26041504 0.92153019 0.43489931 0.1739126  0.07203218]\n",
            " [0.49903593 0.40291537 0.95332648 0.83176616 0.20560173]\n",
            " [0.82791787 0.88214472 0.02808915 0.15691164 0.9241367 ]\n",
            " [0.20717931 0.67133342 0.69909968 0.09542061 0.1366706 ]\n",
            " [0.04378205 0.94071857 0.55442289 0.75814693 0.00797407]\n",
            " [0.97692033 0.6096903  0.87129821 0.8135133  0.56908354]\n",
            " [0.29778104 0.55187308 0.35339633 0.72495755 0.10319394]\n",
            " [0.38649831 0.6186115  0.7578479  0.70111794 0.40498143]\n",
            " [0.43624882 0.39230823 0.39652264 0.81799001 0.9861178 ]]\n",
            "[[0.26041504 0.92153019 0.43489931 0.1739126  0.07203218]\n",
            " [0.49903593 0.40291537 0.95332648 0.83176616 0.20560173]\n",
            " [0.82791787 0.88214472 0.02808915 0.15691164 0.9241367 ]\n",
            " [0.20717931 0.67133342 0.69909968 0.09542061 0.1366706 ]\n",
            " [0.04378205 0.94071857 0.55442289 0.75814693 0.00797407]\n",
            " [0.97692033 0.6096903  0.87129821 0.8135133  0.56908354]\n",
            " [0.29778104 0.55187308 0.35339633 0.72495755 0.10319394]\n",
            " [0.38649831 0.6186115  0.7578479  0.70111794 0.40498143]\n",
            " [0.43547434 0.39330397 0.39596304 0.81795118 0.98511822]]\n",
            "[[0.26136615 0.92099433 0.43437595 0.17422154 0.07227081]\n",
            " [0.49903593 0.40291537 0.95332648 0.83176616 0.20560173]\n",
            " [0.82791787 0.88214472 0.02808915 0.15691164 0.9241367 ]\n",
            " [0.20717931 0.67133342 0.69909968 0.09542061 0.1366706 ]\n",
            " [0.04378205 0.94071857 0.55442289 0.75814693 0.00797407]\n",
            " [0.97692033 0.6096903  0.87129821 0.8135133  0.56908354]\n",
            " [0.29778104 0.55187308 0.35339633 0.72495755 0.10319394]\n",
            " [0.38649831 0.6186115  0.7578479  0.70111794 0.40498143]\n",
            " [0.43547434 0.39330397 0.39596304 0.81795118 0.98511822]]\n",
            "[[0.26136615 0.92099433 0.43437595 0.17422154 0.07227081]\n",
            " [0.49833726 0.40372632 0.9533447  0.83135352 0.20555007]\n",
            " [0.82791787 0.88214472 0.02808915 0.15691164 0.9241367 ]\n",
            " [0.20717931 0.67133342 0.69909968 0.09542061 0.1366706 ]\n",
            " [0.04378205 0.94071857 0.55442289 0.75814693 0.00797407]\n",
            " [0.97692033 0.6096903  0.87129821 0.8135133  0.56908354]\n",
            " [0.29778104 0.55187308 0.35339633 0.72495755 0.10319394]\n",
            " [0.38649831 0.6186115  0.7578479  0.70111794 0.40498143]\n",
            " [0.43547434 0.39330397 0.39596304 0.81795118 0.98511822]]\n",
            "[[0.26136615 0.92099433 0.43437595 0.17422154 0.07227081]\n",
            " [0.49833726 0.40372632 0.9533447  0.83135352 0.20555007]\n",
            " [0.82682957 0.88256755 0.02819103 0.15635655 0.92398791]\n",
            " [0.20717931 0.67133342 0.69909968 0.09542061 0.1366706 ]\n",
            " [0.04378205 0.94071857 0.55442289 0.75814693 0.00797407]\n",
            " [0.97692033 0.6096903  0.87129821 0.8135133  0.56908354]\n",
            " [0.29778104 0.55187308 0.35339633 0.72495755 0.10319394]\n",
            " [0.38649831 0.6186115  0.7578479  0.70111794 0.40498143]\n",
            " [0.43547434 0.39330397 0.39596304 0.81795118 0.98511822]]\n",
            "[[0.26136615 0.92099433 0.43437595 0.17422154 0.07227081]\n",
            " [0.49833726 0.40372632 0.9533447  0.83135352 0.20555007]\n",
            " [0.82682957 0.88256755 0.02819103 0.15635655 0.92398791]\n",
            " [0.20820183 0.6705145  0.69808164 0.09589399 0.13621299]\n",
            " [0.04378205 0.94071857 0.55442289 0.75814693 0.00797407]\n",
            " [0.97692033 0.6096903  0.87129821 0.8135133  0.56908354]\n",
            " [0.29778104 0.55187308 0.35339633 0.72495755 0.10319394]\n",
            " [0.38649831 0.6186115  0.7578479  0.70111794 0.40498143]\n",
            " [0.43547434 0.39330397 0.39596304 0.81795118 0.98511822]]\n",
            "[[0.26136615 0.92099433 0.43437595 0.17422154 0.07227081]\n",
            " [0.49833726 0.40372632 0.9533447  0.83135352 0.20555007]\n",
            " [0.82682957 0.88256755 0.02819103 0.15635655 0.92398791]\n",
            " [0.20820183 0.6705145  0.69808164 0.09589399 0.13621299]\n",
            " [0.04410424 0.94071587 0.55373571 0.75836302 0.00719669]\n",
            " [0.97692033 0.6096903  0.87129821 0.8135133  0.56908354]\n",
            " [0.29778104 0.55187308 0.35339633 0.72495755 0.10319394]\n",
            " [0.38649831 0.6186115  0.7578479  0.70111794 0.40498143]\n",
            " [0.43547434 0.39330397 0.39596304 0.81795118 0.98511822]]\n",
            "[[0.26136615 0.92099433 0.43437595 0.17422154 0.07227081]\n",
            " [0.49833726 0.40372632 0.9533447  0.83135352 0.20555007]\n",
            " [0.82682957 0.88256755 0.02819103 0.15635655 0.92398791]\n",
            " [0.20820183 0.6705145  0.69808164 0.09589399 0.13621299]\n",
            " [0.04410424 0.94071587 0.55373571 0.75836302 0.00719669]\n",
            " [0.97602404 0.61006141 0.87115535 0.81323025 0.5684428 ]\n",
            " [0.29778104 0.55187308 0.35339633 0.72495755 0.10319394]\n",
            " [0.38649831 0.6186115  0.7578479  0.70111794 0.40498143]\n",
            " [0.43547434 0.39330397 0.39596304 0.81795118 0.98511822]]\n",
            "[[0.26231718 0.92046145 0.43385512 0.17453096 0.07250983]\n",
            " [0.49833726 0.40372632 0.9533447  0.83135352 0.20555007]\n",
            " [0.82682957 0.88256755 0.02819103 0.15635655 0.92398791]\n",
            " [0.20820183 0.6705145  0.69808164 0.09589399 0.13621299]\n",
            " [0.04410424 0.94071587 0.55373571 0.75836302 0.00719669]\n",
            " [0.97602404 0.61006141 0.87115535 0.81323025 0.5684428 ]\n",
            " [0.29778104 0.55187308 0.35339633 0.72495755 0.10319394]\n",
            " [0.38649831 0.6186115  0.7578479  0.70111794 0.40498143]\n",
            " [0.43547434 0.39330397 0.39596304 0.81795118 0.98511822]]\n",
            "[[0.26231718 0.92046145 0.43385512 0.17453096 0.07250983]\n",
            " [0.49764194 0.40453908 0.95336724 0.83094455 0.20550109]\n",
            " [0.82682957 0.88256755 0.02819103 0.15635655 0.92398791]\n",
            " [0.20820183 0.6705145  0.69808164 0.09589399 0.13621299]\n",
            " [0.04410424 0.94071587 0.55373571 0.75836302 0.00719669]\n",
            " [0.97602404 0.61006141 0.87115535 0.81323025 0.5684428 ]\n",
            " [0.29778104 0.55187308 0.35339633 0.72495755 0.10319394]\n",
            " [0.38649831 0.6186115  0.7578479  0.70111794 0.40498143]\n",
            " [0.43547434 0.39330397 0.39596304 0.81795118 0.98511822]]\n",
            "[[0.26231718 0.92046145 0.43385512 0.17453096 0.07250983]\n",
            " [0.49764194 0.40453908 0.95336724 0.83094455 0.20550109]\n",
            " [0.82574731 0.88299273 0.02829597 0.15580449 0.92384387]\n",
            " [0.20820183 0.6705145  0.69808164 0.09589399 0.13621299]\n",
            " [0.04410424 0.94071587 0.55373571 0.75836302 0.00719669]\n",
            " [0.97602404 0.61006141 0.87115535 0.81323025 0.5684428 ]\n",
            " [0.29778104 0.55187308 0.35339633 0.72495755 0.10319394]\n",
            " [0.38649831 0.6186115  0.7578479  0.70111794 0.40498143]\n",
            " [0.43547434 0.39330397 0.39596304 0.81795118 0.98511822]]\n",
            "[[0.26231718 0.92046145 0.43385512 0.17453096 0.07250983]\n",
            " [0.49764194 0.40453908 0.95336724 0.83094455 0.20550109]\n",
            " [0.82574731 0.88299273 0.02829597 0.15580449 0.92384387]\n",
            " [0.20922476 0.66970079 0.69706902 0.09636824 0.13575769]\n",
            " [0.04410424 0.94071587 0.55373571 0.75836302 0.00719669]\n",
            " [0.97602404 0.61006141 0.87115535 0.81323025 0.5684428 ]\n",
            " [0.29778104 0.55187308 0.35339633 0.72495755 0.10319394]\n",
            " [0.38649831 0.6186115  0.7578479  0.70111794 0.40498143]\n",
            " [0.43547434 0.39330397 0.39596304 0.81795118 0.98511822]]\n",
            "[[0.26231718 0.92046145 0.43385512 0.17453096 0.07250983]\n",
            " [0.49764194 0.40453908 0.95336724 0.83094455 0.20550109]\n",
            " [0.82574731 0.88299273 0.02829597 0.15580449 0.92384387]\n",
            " [0.20922476 0.66970079 0.69706902 0.09636824 0.13575769]\n",
            " [0.04447382 0.94043051 0.55356707 0.75870674 0.00657105]\n",
            " [0.97602404 0.61006141 0.87115535 0.81323025 0.5684428 ]\n",
            " [0.29778104 0.55187308 0.35339633 0.72495755 0.10319394]\n",
            " [0.38649831 0.6186115  0.7578479  0.70111794 0.40498143]\n",
            " [0.43547434 0.39330397 0.39596304 0.81795118 0.98511822]]\n",
            "[[0.26231718 0.92046145 0.43385512 0.17453096 0.07250983]\n",
            " [0.49764194 0.40453908 0.95336724 0.83094455 0.20550109]\n",
            " [0.82574731 0.88299273 0.02829597 0.15580449 0.92384387]\n",
            " [0.20922476 0.66970079 0.69706902 0.09636824 0.13575769]\n",
            " [0.04447382 0.94043051 0.55356707 0.75870674 0.00657105]\n",
            " [0.9747875  0.61085162 0.87109914 0.81315652 0.56737771]\n",
            " [0.29778104 0.55187308 0.35339633 0.72495755 0.10319394]\n",
            " [0.38649831 0.6186115  0.7578479  0.70111794 0.40498143]\n",
            " [0.43547434 0.39330397 0.39596304 0.81795118 0.98511822]]\n",
            "[[0.26231718 0.92046145 0.43385512 0.17453096 0.07250983]\n",
            " [0.49764194 0.40453908 0.95336724 0.83094455 0.20550109]\n",
            " [0.82574731 0.88299273 0.02829597 0.15580449 0.92384387]\n",
            " [0.20922476 0.66970079 0.69706902 0.09636824 0.13575769]\n",
            " [0.04447382 0.94043051 0.55356707 0.75870674 0.00657105]\n",
            " [0.9747875  0.61085162 0.87109914 0.81315652 0.56737771]\n",
            " [0.29704264 0.55248635 0.35267732 0.72495973 0.10187643]\n",
            " [0.38649831 0.6186115  0.7578479  0.70111794 0.40498143]\n",
            " [0.43547434 0.39330397 0.39596304 0.81795118 0.98511822]]\n",
            "[[0.26231718 0.92046145 0.43385512 0.17453096 0.07250983]\n",
            " [0.49764194 0.40453908 0.95336724 0.83094455 0.20550109]\n",
            " [0.82574731 0.88299273 0.02829597 0.15580449 0.92384387]\n",
            " [0.20922476 0.66970079 0.69706902 0.09636824 0.13575769]\n",
            " [0.04447382 0.94043051 0.55356707 0.75870674 0.00657105]\n",
            " [0.9747875  0.61085162 0.87109914 0.81315652 0.56737771]\n",
            " [0.29704264 0.55248635 0.35267732 0.72495973 0.10187643]\n",
            " [0.38661427 0.61832569 0.75781908 0.70125062 0.40440873]\n",
            " [0.43547434 0.39330397 0.39596304 0.81795118 0.98511822]]\n",
            "[[0.26231718 0.92046145 0.43385512 0.17453096 0.07250983]\n",
            " [0.49764194 0.40453908 0.95336724 0.83094455 0.20550109]\n",
            " [0.82574731 0.88299273 0.02829597 0.15580449 0.92384387]\n",
            " [0.20965191 0.66944448 0.69718692 0.09669126 0.13583827]\n",
            " [0.04447382 0.94043051 0.55356707 0.75870674 0.00657105]\n",
            " [0.9747875  0.61085162 0.87109914 0.81315652 0.56737771]\n",
            " [0.29704264 0.55248635 0.35267732 0.72495973 0.10187643]\n",
            " [0.38661427 0.61832569 0.75781908 0.70125062 0.40440873]\n",
            " [0.43547434 0.39330397 0.39596304 0.81795118 0.98511822]]\n",
            "[[0.26231718 0.92046145 0.43385512 0.17453096 0.07250983]\n",
            " [0.49764194 0.40453908 0.95336724 0.83094455 0.20550109]\n",
            " [0.82574731 0.88299273 0.02829597 0.15580449 0.92384387]\n",
            " [0.20965191 0.66944448 0.69718692 0.09669126 0.13583827]\n",
            " [0.04447382 0.94043051 0.55356707 0.75870674 0.00657105]\n",
            " [0.9747875  0.61085162 0.87109914 0.81315652 0.56737771]\n",
            " [0.29704264 0.55248635 0.35267732 0.72495973 0.10187643]\n",
            " [0.38661427 0.61832569 0.75781908 0.70125062 0.40440873]\n",
            " [0.43470647 0.39430116 0.39540865 0.81791854 0.98412578]]\n",
            "[[0.26326668 0.91992924 0.4333349  0.17483813 0.07274862]\n",
            " [0.49764194 0.40453908 0.95336724 0.83094455 0.20550109]\n",
            " [0.82574731 0.88299273 0.02829597 0.15580449 0.92384387]\n",
            " [0.20965191 0.66944448 0.69718692 0.09669126 0.13583827]\n",
            " [0.04447382 0.94043051 0.55356707 0.75870674 0.00657105]\n",
            " [0.9747875  0.61085162 0.87109914 0.81315652 0.56737771]\n",
            " [0.29704264 0.55248635 0.35267732 0.72495973 0.10187643]\n",
            " [0.38661427 0.61832569 0.75781908 0.70125062 0.40440873]\n",
            " [0.43470647 0.39430116 0.39540865 0.81791854 0.98412578]]\n",
            "[[0.26326668 0.91992924 0.4333349  0.17483813 0.07274862]\n",
            " [0.49694963 0.40535144 0.95339192 0.83053772 0.20545598]\n",
            " [0.82574731 0.88299273 0.02829597 0.15580449 0.92384387]\n",
            " [0.20965191 0.66944448 0.69718692 0.09669126 0.13583827]\n",
            " [0.04447382 0.94043051 0.55356707 0.75870674 0.00657105]\n",
            " [0.9747875  0.61085162 0.87109914 0.81315652 0.56737771]\n",
            " [0.29704264 0.55248635 0.35267732 0.72495973 0.10187643]\n",
            " [0.38661427 0.61832569 0.75781908 0.70125062 0.40440873]\n",
            " [0.43470647 0.39430116 0.39540865 0.81791854 0.98412578]]\n",
            "[[0.26326668 0.91992924 0.4333349  0.17483813 0.07274862]\n",
            " [0.49694963 0.40535144 0.95339192 0.83053772 0.20545598]\n",
            " [0.82467135 0.88341826 0.02840281 0.1552549  0.92370632]\n",
            " [0.20965191 0.66944448 0.69718692 0.09669126 0.13583827]\n",
            " [0.04447382 0.94043051 0.55356707 0.75870674 0.00657105]\n",
            " [0.9747875  0.61085162 0.87109914 0.81315652 0.56737771]\n",
            " [0.29704264 0.55248635 0.35267732 0.72495973 0.10187643]\n",
            " [0.38661427 0.61832569 0.75781908 0.70125062 0.40440873]\n",
            " [0.43470647 0.39430116 0.39540865 0.81791854 0.98412578]]\n",
            "[[0.26326668 0.91992924 0.4333349  0.17483813 0.07274862]\n",
            " [0.49694963 0.40535144 0.95339192 0.83053772 0.20545598]\n",
            " [0.82467135 0.88341826 0.02840281 0.1552549  0.92370632]\n",
            " [0.21067343 0.66863426 0.69617786 0.0971638  0.13538509]\n",
            " [0.04447382 0.94043051 0.55356707 0.75870674 0.00657105]\n",
            " [0.9747875  0.61085162 0.87109914 0.81315652 0.56737771]\n",
            " [0.29704264 0.55248635 0.35267732 0.72495973 0.10187643]\n",
            " [0.38661427 0.61832569 0.75781908 0.70125062 0.40440873]\n",
            " [0.43470647 0.39430116 0.39540865 0.81791854 0.98412578]]\n",
            "[[0.26326668 0.91992924 0.4333349  0.17483813 0.07274862]\n",
            " [0.49694963 0.40535144 0.95339192 0.83053772 0.20545598]\n",
            " [0.82467135 0.88341826 0.02840281 0.1552549  0.92370632]\n",
            " [0.21067343 0.66863426 0.69617786 0.0971638  0.13538509]\n",
            " [0.04479855 0.94043449 0.55288979 0.75892862 0.00579894]\n",
            " [0.9747875  0.61085162 0.87109914 0.81315652 0.56737771]\n",
            " [0.29704264 0.55248635 0.35267732 0.72495973 0.10187643]\n",
            " [0.38661427 0.61832569 0.75781908 0.70125062 0.40440873]\n",
            " [0.43470647 0.39430116 0.39540865 0.81791854 0.98412578]]\n",
            "[[0.26326668 0.91992924 0.4333349  0.17483813 0.07274862]\n",
            " [0.49694963 0.40535144 0.95339192 0.83053772 0.20545598]\n",
            " [0.82467135 0.88341826 0.02840281 0.1552549  0.92370632]\n",
            " [0.21067343 0.66863426 0.69617786 0.0971638  0.13538509]\n",
            " [0.04479855 0.94043449 0.55288979 0.75892862 0.00579894]\n",
            " [0.9739012  0.61122736 0.87096374 0.81288067 0.56674687]\n",
            " [0.29704264 0.55248635 0.35267732 0.72495973 0.10187643]\n",
            " [0.38661427 0.61832569 0.75781908 0.70125062 0.40440873]\n",
            " [0.43470647 0.39430116 0.39540865 0.81791854 0.98412578]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7I6U3J46Brf0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    # Run through first matrix (w1) to get hidden layer - 10x9 dot 9x1 gives us 10x1\n",
        "    h = np.dot(self.w1.T, x)\n",
        "    # Dot product hidden layer with second matrix (w2) - 9x10 dot 10x1 gives us 9x1\n",
        "    u = np.dot(self.w2.T, h)\n",
        "    # Run 1x9 through softmax to force each element to range of [0, 1] - 1x8\n",
        "    y_c = self.softmax(u)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5kjS4ts9DMv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  for i in range(self.epochs):\n",
        "    self.loss = 0\n",
        "    for w_t, w_c in training_data:\n",
        "      EI = np.sum([np.subtract(y_pred, word) for word in w_c], axis=0)\n",
        "      self.backprop(EI, h, w_t)\n",
        "\n",
        "      # Calculate loss\n",
        "      # There are 2 parts to the loss function\n",
        "      # Part 1: -ve sum of all the output +\n",
        "      # Part 2: length of context words * log of sum for all elements (exponential-ed) in the output layer before softmax (u)\n",
        "      # Note: word.index(1) returns the index in the context word vector with value 1\n",
        "      # Note: u[word.index(1)] returns the value of the output layer before softmax\n",
        "      self.loss += -np.sum([u[word.index(1)] for word in w_c]) + len(w_c) * np.log(np.sum(np.exp(u)))\n",
        "    print('Epoch:', i, \"Loss:\", self.loss)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}